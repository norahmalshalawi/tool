{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install SpeechRecognition moviepy\n",
        "\n",
        "import speech_recognition as sr\n",
        "import moviepy.editor as mp\n",
        "video=\"/content/video (4).mp4\" #The Answer (as video) frm storge\n",
        "clip = mp.VideoFileClip(video)\n",
        "\n",
        "clip_audio = clip.audio\n",
        "\n",
        "clip_audio.write_audiofile('audio.wav')\n",
        "\n",
        "clip_audio.write_audiofile('audio.wav')\n",
        "\n",
        "r = sr.Recognizer()\n",
        "\n",
        "with sr.AudioFile('audio.wav') as source:\n",
        "    audio = r.record(source)\n",
        "    text = r.recognize_google(audio)#The Answer converted (as text)\n",
        "print(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jzFb9j7RL5Sb",
        "outputId": "099b5b42-e553-4c17-da89-741073bff748"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: SpeechRecognition in /usr/local/lib/python3.10/dist-packages (3.10.0)\n",
            "Requirement already satisfied: moviepy in /usr/local/lib/python3.10/dist-packages (1.0.3)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from SpeechRecognition) (2.27.1)\n",
            "Requirement already satisfied: decorator<5.0,>=4.0.2 in /usr/local/lib/python3.10/dist-packages (from moviepy) (4.4.2)\n",
            "Requirement already satisfied: tqdm<5.0,>=4.11.2 in /usr/local/lib/python3.10/dist-packages (from moviepy) (4.65.0)\n",
            "Requirement already satisfied: proglog<=1.0.0 in /usr/local/lib/python3.10/dist-packages (from moviepy) (0.1.10)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from moviepy) (1.22.4)\n",
            "Requirement already satisfied: imageio<3.0,>=2.5 in /usr/local/lib/python3.10/dist-packages (from moviepy) (2.25.1)\n",
            "Requirement already satisfied: imageio-ffmpeg>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from moviepy) (0.4.8)\n",
            "Requirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.10/dist-packages (from imageio<3.0,>=2.5->moviepy) (9.4.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->SpeechRecognition) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->SpeechRecognition) (2023.7.22)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->SpeechRecognition) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->SpeechRecognition) (3.4)\n",
            "MoviePy - Writing audio in audio.wav\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MoviePy - Done.\n",
            "MoviePy - Writing audio in audio.wav\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                                                        "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MoviePy - Done.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "blinds schedule management define activities estimate activity durations sequence activities control schedule develops schedule\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "random_q=[\"/content/Risk.txt\",\"/content/Schedule.txt\",\"/content/Cost.txt\"]\n",
        "random_number = random.randint(0,2)\n",
        "pathD=random_q[random_number]\n",
        "print(pathD)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oJSWynM8NfB_",
        "outputId": "0c87b7c3-8fb7-46e5-f21c-052983375d0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Schedule.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc1= open(pathD) #you could change it to be input from user as txt file\n",
        "doc1_read=doc1.read()\n",
        "#To extract the Q\n",
        "# Open the file for reading\n",
        "with open(pathD, \"r\") as file:\n",
        "    # Read the first line of the file\n",
        "    first_line = file.readline()\n",
        "    # Print the first line\n",
        "    print(first_line)\n",
        "\n",
        "#lower case\n",
        "low=doc1_read.lower()\n",
        "#remove punctuation\n",
        "punc = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n",
        "for ele in low:\n",
        "  if ele in punc:\n",
        "   low=low.replace(ele, \"\")\n",
        "#remove number\n",
        "remove = '12345690'\n",
        "for ele in low:\n",
        "  if ele in remove:\n",
        "   low = low.replace(ele, \"\")\n",
        "#tokenization\n",
        "tok=word_tokenize(low)\n",
        "#stopword removal\n",
        "all_stopwords=stopwords.words('english')\n",
        "all_stopwords.append('play')\n",
        "tokens_without_sw= [word for word in tok if not word in all_stopwords]\n",
        "##lemmatization\n",
        "lemmatiz=WordNetLemmatizer()\n",
        "tx=[lemmatiz.lemmatize(word) for word in tokens_without_sw]\n",
        "print(tx)\n",
        "Doc01= ' '.join(map( str, tx ))\n",
        "l=Doc01.split()\n",
        "s=set(l)\n",
        "Doc001=' '.join(s)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GpQFuAfuE0fB",
        "outputId": "88851bcb-8142-4bcd-a01c-6922b17280cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Could you mention the processes in project Schedule Management, please?\n",
            "\n",
            "['could', 'mention', 'process', 'project', 'schedule', 'management', 'please', 'plan', 'schedule', 'management', 'define', 'activity', 'sequence', 'activity', 'estimate', 'activity', 'duration', 'develop', 'schedule', 'control', 'schedule']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#pre-prosssesing for the answer\n",
        "doc1=text\n",
        "doc1_read=text\n",
        "#lower case\n",
        "low=doc1_read.lower()\n",
        "#remove punctuation\n",
        "punc = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n",
        "for ele in low:\n",
        "  if ele in punc:\n",
        "   low=low.replace(ele, \"\")\n",
        "#remove number\n",
        "remove = '12345690'\n",
        "for ele in low:\n",
        "  if ele in remove:\n",
        "   low = low.replace(ele, \"\")\n",
        "#tokenization\n",
        "tok=word_tokenize(low)\n",
        "#stopword removal\n",
        "all_stopwords=stopwords.words('english')\n",
        "all_stopwords.append('play')\n",
        "tokens_without_sw= [word for word in tok if not word in all_stopwords]\n",
        "##lemmatization\n",
        "lemmatiz=WordNetLemmatizer()\n",
        "text=[lemmatiz.lemmatize(word) for word in tokens_without_sw]\n",
        "Doc02= ' '.join(map( str, text ))\n",
        "l=Doc02.split()\n",
        "s=set(l)\n",
        "Doc002=' '.join(s)\n"
      ],
      "metadata": {
        "id": "XmBxorYYGLcH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# documents\n",
        "\n",
        "doc1=Doc001 #the knowledge\n",
        "doc2=Doc002 #the answer\n",
        "\n",
        "# vectorize the documents using TF-IDF\n",
        "vectorizer = TfidfVectorizer()\n",
        "doc1_vec = vectorizer.fit_transform([doc1])\n",
        "doc2_vec = vectorizer.transform([doc2])\n",
        "\n",
        "# calculate cosine similarity between the documents\n",
        "similarity_score = cosine_similarity(doc1_vec, doc2_vec)\n",
        "result=similarity_score[0][0]\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sUoK6L6MGnI0",
        "outputId": "8b7dc507-12b8-46af-b597-7f7730a8277a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7302967433402215\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pass=False #by defuilt fall\n",
        "pres_pass=0.7 # you can ask the user to enter %\n",
        "if result>pres_pass:\n",
        "   is_pass=True\n",
        "\n",
        "print(pass)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "We-dE7bqSYBp",
        "outputId": "f19a45b2-ebd8-4045-b122-21ca0b5d66fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-f44fccc2e592>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    pass=False #by defuilt fall\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MxK1WY4dEgMf",
        "outputId": "3b69bc19-7857-4395-acd6-3e1082d96ee2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.6)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2022.10.31)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.65.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "! pip install nltk\n",
        "## Lower case\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "## Tokenization\n",
        "from nltk.tokenize import word_tokenize\n",
        "import string\n",
        "\n",
        "##Stopword removal\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "sw_nltk = stopwords.words('english')\n",
        "\n",
        "##Lemmatization and stemming\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "\n",
        "## TF-IDF\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n"
      ]
    }
  ]
}